# -*- coding: utf-8 -*-
"""
GitHub High-Performance Data Collector (Enterprise Edition)
-----------------------------------------------------------
æ ¸å¿ƒä¼˜åŠ¿ï¼š
1. GraphQL é©±åŠ¨ï¼šç›¸æ¯” REST APIï¼Œæ•°æ®é‡‡é›†æ•ˆç‡æå‡çº¦ 8-15 å€ã€‚
2. å·¥ä¸šçº§å­˜å‚¨ï¼šåŸºäº SQLite WAL æ¨¡å¼ï¼Œæ”¯æŒæµ·é‡æ•°æ®é«˜å¹¶å‘å†™å…¥ä¸å»é‡ã€‚
3. æ™ºèƒ½å®¹é”™ï¼šå†…ç½® Token ç†”æ–­æœºåˆ¶ï¼Œè‡ªåŠ¨å¤„ç† GitHub API é™æµã€‚
4. æ–­ç‚¹ç»­ä¼ ï¼šæ”¯æŒéšæ—¶åœæ­¢ä»»åŠ¡ï¼Œä¸‹æ¬¡è¿è¡Œè‡ªåŠ¨ä»æ–­ç‚¹å¤„ç»§ç»­ã€‚

å®‰è£…ä¾èµ–: pip install aiohttp tqdm
"""

import asyncio
import csv
import json
import logging
import os
import signal
import sqlite3
import sys
import time
from dataclasses import asdict, dataclass, field
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

# æ£€æŸ¥ç¬¬ä¸‰æ–¹åº“
try:
    import aiohttp
    from tqdm.asyncio import tqdm
except ImportError:
    print("é”™è¯¯: ç¼ºå°‘å¿…è¦ä¾èµ–ã€‚è¯·æ‰§è¡Œ: pip install aiohttp tqdm")
    sys.exit(1)

# ======================== é…ç½®ä¸­å¿ƒ ========================

class Config:
    GITHUB_GRAPHQL_URL = "https://api.github.com/graphql"
    DB_NAME = "github_data_center.db"
    LOG_FILE = "gh_collector.log"
    DEFAULT_TIMEOUT = aiohttp.ClientTimeout(total=60)
    BATCH_SIZE = 100  # æ•°æ®åº“æ‰¹é‡å†™å…¥é˜ˆå€¼
    MAX_RETRIES = 5   # æœ€å¤§é‡è¯•æ¬¡æ•°

    # GraphQL æŸ¥è¯¢æ¨¡æ¿ï¼šä¸€æ¬¡æ€§è·å–æäº¤å…ƒæ•°æ®ã€ä½œè€…è´¦å·åŠä½ç½®ä¿¡æ¯
    GRAPHQL_QUERY = """
    query($owner: String!, $name: String!, $since: GitTimestamp, $until: GitTimestamp, $cursor: String) {
      repository(owner: $owner, name: $name) {
        defaultBranchRef {
          target {
            ... on Commit {
              history(first: 100, since: $since, until: $until, after: $cursor) {
                pageInfo {
                  endCursor
                  hasNextPage
                }
                edges {
                  node {
                    oid
                    message
                    committedDate
                    author {
                      name
                      email
                      user {
                        login
                        location
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
    """

@dataclass
class CommitRecord:
    repo_name: str
    commit_sha: str
    timestamp_unix: int
    author_login: str
    author_name: str
    author_email: str
    location: str
    message: str
    collected_at: str = field(default_factory=lambda: datetime.now().isoformat())

    def to_flat_dict(self) -> Dict[str, Any]:
        """å±•å¹³æ•°æ®ç”¨äºå¯¼å‡º"""
        d = asdict(self)
        # æ¸…æ´—æ¢è¡Œç¬¦ï¼Œé˜²æ­¢ CSV æ ¼å¼å´©æºƒ
        d['message'] = d['message'].replace('\n', ' ').replace('\r', '')[:200]
        d['location'] = (d['location'] or "").replace('\n', ' ').strip()
        return d

# ======================== åŸºç¡€è®¾æ–½å±‚ ========================

class DatabaseManager:
    """ç®¡ç† SQLite å­˜å‚¨ä¸å»é‡é€»è¾‘"""
    def __init__(self, db_path: str = Config.DB_NAME):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        with sqlite3.connect(self.db_path) as conn:
            # å¼€å¯ WAL æ¨¡å¼æé«˜å¹¶å‘æ€§èƒ½
            conn.execute("PRAGMA journal_mode=WAL")
            # æäº¤è®°å½•è¡¨ (SHA ä½œä¸ºä¸»é”®å®ç°è‡ªåŠ¨å»é‡)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS commits (
                    sha TEXT PRIMARY KEY,
                    repo TEXT,
                    author_login TEXT,
                    ts_unix INTEGER,
                    data_json TEXT
                )
            """)
            conn.execute("CREATE INDEX IF NOT EXISTS idx_repo ON commits(repo)")
            conn.commit()

    def check_exists(self, sha: str) -> bool:
        """æ£€æŸ¥ SHA æ˜¯å¦å·²å­˜åœ¨"""
        with sqlite3.connect(self.db_path) as conn:
            cur = conn.cursor()
            cur.execute("SELECT 1 FROM commits WHERE sha = ?", (sha,))
            return cur.fetchone() is not None

    def save_batch(self, records: List[CommitRecord]):
        """æ‰¹é‡æŒä¹…åŒ–"""
        if not records:
            return
        with sqlite3.connect(self.db_path) as conn:
            data = [
                (r.commit_sha, r.repo_name, r.author_login, r.timestamp_unix, json.dumps(r.to_flat_dict()))
                for r in records
            ]
            conn.executemany("INSERT OR IGNORE INTO commits VALUES (?,?,?,?,?)", data)
            conn.commit()

class TokenManager:
    """å¸¦é™æµç†”æ–­æœºåˆ¶çš„ Token è°ƒåº¦å™¨"""
    def __init__(self, tokens: List[str]):
        # è®°å½•æ¯ä¸ª Token çš„å†·å´ç»“æŸæ—¶é—´
        self._tokens = {t.strip(): 0.0 for t in tokens if t.strip()}
        if not self._tokens:
            raise ValueError("é”™è¯¯: æœªé…ç½®æœ‰æ•ˆçš„ GitHub Token")
        self._lock = asyncio.Lock()

    async def get_token(self) -> str:
        """è·å–å½“å‰å¯ç”¨çš„ Tokenï¼Œè‹¥å…¨éƒ¨å†·å´åˆ™ç­‰å¾…"""
        async with self._lock:
            while True:
                now = time.time()
                # å¯»æ‰¾ä¸åœ¨å†·å´æœŸçš„ Token
                available = [t for t, cooldown in self._tokens.items() if now >= cooldown]
                if available:
                    # è½®è¯¢æˆ–éšæœºé€‰æ‹©
                    import random
                    return random.choice(available)
                
                wait_time = min(self._tokens.values()) - now + 1
                logging.warning(f"æ‰€æœ‰ Token å‡å¤„äºé™æµå†·å´ä¸­ï¼Œå¼ºåˆ¶ä¼‘çœ  {wait_time:.1f}s...")
                await asyncio.sleep(max(wait_time, 5))

    def mark_limited(self, token: str, duration: int = 60):
        """æ ‡è®° Token è¿›å…¥å†·å´æœŸ (ä¾‹å¦‚è§¦å‘ 403 æˆ– 429)"""
        self._tokens[token] = time.time() + duration
        logging.error(f"Token [{token[:10]}...] è§¦å‘é™æµï¼Œè¿›å…¥ {duration}s å†·å´æœŸ")

# ======================== æ ¸å¿ƒé€»è¾‘å±‚ ========================

class GitHubCollector:
    def __init__(self, token_mgr: TokenManager, db: DatabaseManager, concurrency: int = 3):
        self.token_mgr = token_mgr
        self.db = db
        self.sem = asyncio.Semaphore(concurrency)
        self.is_running = True
        self._setup_signals()

    def _setup_signals(self):
        """ä¼˜é›…é€€å‡ºå¤„ç†"""
        for sig in (signal.SIGINT, signal.SIGTERM):
            try:
                signal.signal(sig, self._handle_exit)
            except ValueError:
                pass

    def _handle_exit(self, *args):
        if self.is_running:
            logging.warning("\n[ç»ˆæ­¢] æ­£åœ¨åœæ­¢ä»»åŠ¡å¹¶ä¿å­˜å·²è·å–çš„æ•°æ®...")
            self.is_running = False

    async def _api_call(self, session: aiohttp.ClientSession, variables: Dict) -> Optional[Dict]:
        """æ‰§è¡Œå¸¦é‡è¯•å’Œç†”æ–­æ§åˆ¶çš„ API è°ƒç”¨"""
        for attempt in range(Config.MAX_RETRIES):
            if not self.is_running: return None
            
            token = await self.token_mgr.get_token()
            headers = {
                "Authorization": f"bearer {token}",
                "Content-Type": "application/json",
                "User-Agent": "GitHub-Pro-Collector-v2"
            }

            try:
                async with session.post(
                    Config.GITHUB_GRAPHQL_URL,
                    json={"query": Config.GRAPHQL_QUERY, "variables": variables},
                    headers=headers,
                    timeout=Config.DEFAULT_TIMEOUT
                ) as resp:
                    if resp.status == 200:
                        res_json = await resp.json()
                        # æ£€æŸ¥ GraphQL å†…éƒ¨é”™è¯¯
                        if "errors" in res_json:
                            err_msg = str(res_json["errors"])
                            if "rate limit" in err_msg.lower() or "RATE_LIMITED" in err_msg:
                                self.token_mgr.mark_limited(token, 300)
                                continue
                            logging.error(f"GraphQL è§£æé”™è¯¯: {err_msg[:200]}")
                            return None
                        return res_json
                    
                    if resp.status in (403, 429):
                        retry_after = int(resp.headers.get("Retry-After", 60))
                        self.token_mgr.mark_limited(token, retry_after)
                        continue
                    
                    logging.warning(f"HTTP {resp.status} å¼‚å¸¸ï¼Œé‡è¯•ä¸­ ({attempt+1}/{Config.MAX_RETRIES})")
            except Exception as e:
                logging.debug(f"è¿æ¥å¼‚å¸¸: {e}")
            
            await asyncio.sleep(2 ** attempt)
        return None

    async def collect_repository(self, repo_full_name: str, since: datetime, until: datetime, pbar: tqdm):
        """é‡‡é›†å•ä¸ªä»“åº“"""
        if "/" not in repo_full_name: return
        owner, name = repo_full_name.split("/")
        
        variables = {
            "owner": owner,
            "name": name,
            "since": since.isoformat(),
            "until": until.isoformat(),
            "cursor": None
        }

        async with self.sem:
            async with aiohttp.ClientSession() as session:
                while self.is_running:
                    data = await self._api_call(session, variables)
                    if not data: break

                    try:
                        repo_data = data.get("data", {}).get("repository")
                        if not repo_data or not repo_data.get("defaultBranchRef"):
                            logging.warning(f"[{repo_full_name}] ä»“åº“ä¸å­˜åœ¨ã€ä¸ºç©ºæˆ–æ— æƒè®¿é—®")
                            break

                        history = repo_data["defaultBranchRef"]["target"]["history"]
                        edges = history.get("edges", [])
                        
                        current_batch = []
                        for edge in edges:
                            node = edge["node"]
                            sha = node["oid"]

                            # æ–­ç‚¹ç»­ä¼ ï¼šå¦‚æœæ•°æ®åº“å·²æœ‰æ­¤ SHAï¼Œåˆ™è·³è¿‡
                            if self.db.check_exists(sha):
                                continue

                            author_info = node.get("author", {})
                            user_node = author_info.get("user") or {}

                            record = CommitRecord(
                                repo_name=repo_full_name,
                                commit_sha=sha,
                                timestamp_unix=int(datetime.fromisoformat(node["committedDate"].replace("Z", "+00:00")).timestamp()),
                                author_login=user_node.get("login") or "ghost-user",
                                author_name=author_info.get("name") or "Unknown",
                                author_email=author_info.get("email") or "",
                                location=user_node.get("location") or "",
                                message=node["message"]
                            )
                            current_batch.append(record)

                        # æ‰¹é‡ä¿å­˜
                        self.db.save_batch(current_batch)
                        pbar.update(len(current_batch))

                        # åˆ†é¡µé€»è¾‘
                        page_info = history.get("pageInfo", {})
                        if page_info.get("hasNextPage") and self.is_running:
                            variables["cursor"] = page_info.get("endCursor")
                        else:
                            break
                    except Exception as e:
                        logging.error(f"[{repo_full_name}] è§£æå¼‚å¸¸: {e}")
                        break

# ======================== å·¥å…·ä¸è¿è¡Œå±‚ ========================

class Reporter:
    @staticmethod
    def generate_csv(db_path: str, output_path: str):
        """å°† SQLite æ•°æ®å¯¼å‡ºä¸º CSV"""
        with sqlite3.connect(db_path) as conn:
            cur = conn.cursor()
            cur.execute("SELECT data_json FROM commits")
            rows = cur.fetchall()
            
            if not rows:
                print("è­¦å‘Š: æ•°æ®åº“ä¸­æ²¡æœ‰å¯å¯¼å‡ºçš„æ•°æ®")
                return

            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, 'w', encoding='utf-8-sig', newline='') as f:
                first_row = json.loads(rows[0][0])
                writer = csv.DictWriter(f, fieldnames=first_row.keys())
                writer.writeheader()
                for r in rows:
                    writer.writerow(json.loads(r[0]))
        
        print(f"âœ… æ•°æ®æˆåŠŸå¯¼å‡ºè‡³: {output_path}")

    @staticmethod
    def generate_stats(db_path: str):
        """ç”Ÿæˆç»Ÿè®¡ç®€æŠ¥"""
        with sqlite3.connect(db_path) as conn:
            cur = conn.cursor()
            cur.execute("SELECT count(*), count(CASE WHEN json_extract(data_json, '$.location') != '' THEN 1 END) FROM commits")
            total, with_loc = cur.fetchone()
            
            print("\n" + "="*30)
            print(f"é‡‡é›†æ¦‚è§ˆæŠ¥å‘Š")
            print(f"æ€»è®¡è®°å½•æ•°: {total}")
            print(f"æœ‰æ•ˆä½ç½®æ•°: {with_loc}")
            print(f"ä½ç½®è¦†ç›–ç‡: {(with_loc/total*100 if total > 0 else 0):.2f}%")
            print("="*30 + "\n")

async def main():
    import argparse
    parser = argparse.ArgumentParser(description="GitHub Pro Collector v2")
    parser.add_argument("--repos", nargs="+", required=True, help="ä»“åº“åˆ—è¡¨ (ä¾‹å¦‚: facebook/react)")
    parser.add_argument("--tokens", nargs="+", required=True, help="GitHub Tokens (æ”¯æŒå¤šä¸ª)")
    parser.add_argument("--days", type=int, default=30, help="å›æº¯å¤©æ•°")
    parser.add_argument("--concurrency", type=int, default=3, help="å¹¶å‘ä»“åº“æ•°")
    parser.add_argument("--output", default="output/github_commits.csv", help="CSV è¾“å‡ºè·¯å¾„")
    args = parser.parse_args()

    # åˆå§‹åŒ–æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(message)s",
        handlers=[logging.FileHandler(Config.LOG_FILE), logging.StreamHandler()]
    )

    db = DatabaseManager()
    tm = TokenManager(args.tokens)
    collector = GitHubCollector(tm, db, concurrency=args.concurrency)

    # æ—¶é—´èŒƒå›´
    until = datetime.now(timezone.utc)
    since = until - timedelta(days=args.days)

    print(f"ğŸš€ å¯åŠ¨é‡‡é›†ä»»åŠ¡ | ç›®æ ‡å¤©æ•°: {args.days} | ä»“åº“æ•°: {len(args.repos)}")
    
    start_time = time.time()
    with tqdm(desc="æ­£åœ¨é‡‡é›†æäº¤", unit="æ¡") as pbar:
        tasks = [collector.collect_repository(r, since, until, pbar) for r in args.repos]
        await asyncio.gather(*tasks)
    
    duration = time.time() - start_time
    print(f"\nğŸ‰ é‡‡é›†ç»“æŸï¼Œè€—æ—¶: {duration:.1f}s")
    
    # æŠ¥å‘Šä¸å¯¼å‡º
    Reporter.generate_stats(Config.DB_NAME)
    Reporter.generate_csv(Config.DB_NAME, args.output)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass
