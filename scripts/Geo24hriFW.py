# -*- coding: utf-8 -*-
"""
GitHub é«˜æ€§èƒ½æ•°æ®é‡‡é›†ç³»ç»Ÿ 
-----------------------------------------------------------
æ ¸å¿ƒç‰¹æ€§ï¼š
1. å¼‚æ­¥ DB é©±åŠ¨ï¼šä½¿ç”¨ aiosqlite æ¶ˆé™¤æ•°æ®åº“ I/O é˜»å¡ã€‚
2. ç®¡é“æ¨¡å¼ï¼šåŸºäº asyncio.Queue çš„ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¶æ„ï¼Œæå‡æµ·é‡æ•°æ®ååã€‚
3. æ™ºèƒ½å®¹é”™ï¼šå¤šçº§é‡è¯•ä¸ Token è°ƒåº¦ç®—æ³•ï¼Œè‡ªåŠ¨è§„é¿ Secondary Rate Limitã€‚
4. å†…å­˜ç´¢å¼•ï¼šåŸºäºé›†åˆçš„ç§’çº§å»é‡ï¼Œæ˜¾è‘—é™ä½ç£ç›˜å¼€é”€ã€‚
"""

import asyncio
import csv
import json
import logging
import os
import signal
import sys
import time
from dataclasses import asdict, dataclass, field
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

# ç¬¬ä¸‰æ–¹ä¾èµ–æ£€æŸ¥
try:
    import aiohttp
    import aiosqlite
    from tqdm.asyncio import tqdm
except ImportError:
    print("é”™è¯¯: ç¼ºå°‘å¿…è¦ä¾èµ–ã€‚è¯·æ‰§è¡Œ: pip install aiohttp aiosqlite tqdm")
    sys.exit(1)

# ======================== æ ¸å¿ƒå¼‚å¸¸ä½“ç³» ========================

class GitHubCollectorError(Exception):
    """åŸºç¡€å¼‚å¸¸ç±»"""
    pass

class AuthError(GitHubCollectorError):
    """èº«ä»½éªŒè¯å¤±è´¥"""
    pass

class RateLimitError(GitHubCollectorError):
    """è§¦å‘é™æµ"""
    pass

# ======================== æ•°æ®æ¨¡å‹ä¸é…ç½® ========================

@dataclass(frozen=True)
class CommitRecord:
    """ä¸å¯å˜æäº¤è®°å½•æ¨¡å‹"""
    repo_name: str
    commit_sha: str
    timestamp_unix: int
    author_login: str
    author_name: str
    author_email: str
    location: str
    message: str
    collected_at: str = field(default_factory=lambda: datetime.now(timezone.utc).isoformat())

    def to_db_row(self) -> Tuple:
        return (
            self.commit_sha,
            self.repo_name,
            self.author_login,
            self.timestamp_unix,
            json.dumps(asdict(self))
        )

class AppConfig:
    """ç³»ç»Ÿå…¨å±€é…ç½®"""
    API_URL = "https://api.github.com/graphql"
    DB_PATH = "gh_enterprise_v3.db"
    LOG_FORMAT = "%(asctime)s [%(levelname)s] [%(name)s] %(message)s"
    
    # ç½‘ç»œé…ç½®
    CONCURRENT_REPOS = 5      # åŒæ—¶é‡‡é›†çš„ä»“åº“æ•°é‡
    PAGE_SIZE = 100           # æ¯ä¸€é¡µè·å–çš„è®°å½•æ•°
    MAX_RETRIES = 5           # æœ€å¤§é‡è¯•æ¬¡æ•°
    TIMEOUT = aiohttp.ClientTimeout(total=120, connect=10)
    
    # æ•°æ®åº“é…ç½®
    WRITE_BATCH_SIZE = 200    # ç¼“å†²åŒºè¾¾åˆ°æ­¤æ•°å€¼åè§¦å‘æ‰¹é‡å†™å…¥
    
    GRAPHQL_TEMPLATE = """
    query($owner: String!, $name: String!, $since: GitTimestamp, $until: GitTimestamp, $cursor: String) {
      repository(owner: $owner, name: $name) {
        defaultBranchRef {
          target {
            ... on Commit {
              history(first: %d, since: $since, until: $until, after: $cursor) {
                pageInfo { endCursor hasNextPage }
                edges {
                  node {
                    oid message committedDate
                    author {
                      name email
                      user { login location }
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
    """ % PAGE_SIZE

# ======================== åŸºç¡€è®¾æ–½å±‚ ========================

class TokenPool:
    """é«˜æ€§èƒ½ Token è°ƒåº¦ä¸­å¿ƒ"""
    def __init__(self, tokens: List[str]):
        self._tokens = {t.strip(): 0.0 for t in tokens if t.strip()}
        if not self._tokens:
            raise AuthError("æœªé…ç½®ä»»ä½•æœ‰æ•ˆçš„ GitHub Personal Access Token")
        self._lock = asyncio.Lock()
        self._logger = logging.getLogger("TokenPool")

    async def get_best_token(self) -> str:
        """é€‰æ‹©å†·å´æ—¶é—´æœ€çŸ­çš„å¯ç”¨ Token"""
        async with self._lock:
            while True:
                now = time.time()
                available = [t for t, cooldown in self._tokens.items() if now >= cooldown]
                if available:
                    # è½®è¯¢ç­–ç•¥
                    token = available[0]
                    # å°†å…¶æ’åˆ°æœ«å°¾ä»¥å¹³è¡¡è´Ÿè½½
                    self._tokens.pop(token)
                    self._tokens[token] = 0.0
                    return token
                
                wait_time = min(self._tokens.values()) - now + 0.5
                self._logger.warning(f"æ‰€æœ‰ Token å·²é™æµï¼Œè‡ªåŠ¨æŒ‚èµ· {wait_time:.1f}s")
                await asyncio.sleep(max(wait_time, 1))

    def penalize(self, token: str, duration: int = 600):
        """å¯¹è§¦å‘é”™è¯¯çš„ Token è¿›è¡Œæƒ©ç½šï¼ˆè¿›å…¥å†·å´ï¼‰"""
        self._tokens[token] = time.time() + duration
        self._logger.error(f"Token [{token[:8]}...] è§¦å‘é™æµï¼Œå°é” {duration}s")

class AsyncDatabase:
    """å¼‚æ­¥ SQLite ç®¡ç†å™¨"""
    def __init__(self, db_path: str):
        self.db_path = db_path
        self._conn: Optional[aiosqlite.Connection] = None

    async def connect(self):
        self._conn = await aiosqlite.connect(self.db_path)
        # é«˜çº§æ€§èƒ½ä¼˜åŒ–
        await self._conn.execute("PRAGMA journal_mode=WAL")
        await self._conn.execute("PRAGMA synchronous=NORMAL")
        await self._conn.execute("PRAGMA cache_size=-64000") # 64MB ç¼“å­˜
        
        await self._conn.execute("""
            CREATE TABLE IF NOT EXISTS commits (
                sha TEXT PRIMARY KEY,
                repo TEXT,
                author_login TEXT,
                ts_unix INTEGER,
                raw_json TEXT
            )
        """)
        await self._conn.execute("CREATE INDEX IF NOT EXISTS idx_repo_ts ON commits(repo, ts_unix)")
        await self._conn.commit()

    async def get_known_shas(self, repo: str) -> Set[str]:
        """è·å–æŒ‡å®šä»“åº“å·²å­˜åœ¨çš„ SHA ç¼“å­˜"""
        async with self._conn.execute("SELECT sha FROM commits WHERE repo = ?", (repo,)) as cursor:
            rows = await cursor.fetchall()
            return {row[0] for row in rows}

    async def save_batch(self, records: List[CommitRecord]):
        if not records: return
        data = [r.to_db_row() for r in records]
        await self._conn.executemany("INSERT OR IGNORE INTO commits VALUES (?,?,?,?,?)", data)
        await self._conn.commit()

    async def close(self):
        if self._conn:
            await self._conn.close()

# ======================== æ ¸å¿ƒé€»è¾‘å±‚ ========================

class CollectionEngine:
    """æ•°æ®é‡‡é›†å¼•æ“ (ç”Ÿäº§è€…-æ¶ˆè´¹è€…)"""
    def __init__(self, token_pool: TokenPool, db: AsyncDatabase):
        self.token_pool = token_pool
        self.db = db
        self.data_queue = asyncio.Queue(maxsize=1000)
        self.is_running = True
        self.stats = {"total_saved": 0, "errors": 0}
        self._logger = logging.getLogger("Engine")

    async def _api_request(self, session: aiohttp.ClientSession, variables: dict) -> Optional[dict]:
        """å°è£…é‡è¯•ä¸é™æµé€»è¾‘çš„åŸå­è¯·æ±‚"""
        for attempt in range(AppConfig.MAX_RETRIES):
            if not self.is_running: return None
            
            token = await self.token_pool.get_best_token()
            headers = {
                "Authorization": f"bearer {token}",
                "User-Agent": "GH-Enterprise-Collector-v3"
            }
            
            try:
                async with session.post(AppConfig.API_URL, json={
                    "query": AppConfig.GRAPHQL_TEMPLATE, 
                    "variables": variables
                }, headers=headers, timeout=AppConfig.TIMEOUT) as resp:
                    
                    if resp.status == 200:
                        res_json = await resp.json()
                        if "errors" in res_json:
                            err_msg = str(res_json["errors"])
                            if "rate limit" in err_msg.lower():
                                self.token_pool.penalize(token, 300)
                                continue
                            self._logger.error(f"GraphQL é€»è¾‘é”™è¯¯: {err_msg[:200]}")
                            return None
                        return res_json
                    
                    if resp.status in (403, 429):
                        retry_after = int(resp.headers.get("Retry-After", 60))
                        self.token_pool.penalize(token, retry_after)
                        continue
                        
                    self._logger.warning(f"HTTP {resp.status} é‡è¯•ä¸­ ({attempt+1})")
            except Exception as e:
                self._logger.debug(f"è¿æ¥å¼‚å¸¸: {type(e).__name__}")
            
            await asyncio.sleep(2 ** attempt)
        return None

    async def repository_worker(self, repo_name: str, since: datetime, until: datetime, pbar: tqdm):
        """ç”Ÿäº§è€…ï¼šè´Ÿè´£ä» GitHub æŠ“å–æ•°æ®å¹¶æ¨å…¥é˜Ÿåˆ—"""
        if "/" not in repo_name: return
        owner, name = repo_name.split("/")
        
        # åŠ è½½å†…å­˜å»é‡ç¼“å­˜
        known_shas = await self.db.get_known_shas(repo_name)
        
        variables = {
            "owner": owner, "name": name,
            "since": since.isoformat(), "until": until.isoformat(),
            "cursor": None
        }

        async with aiohttp.ClientSession() as session:
            while self.is_running:
                data = await self._api_request(session, variables)
                if not data: break
                
                try:
                    target = data.get("data", {}).get("repository", {}).get("defaultBranchRef", {}).get("target")
                    if not target: break
                    
                    history = target.get("history", {})
                    edges = history.get("edges", [])
                    
                    batch = []
                    for edge in edges:
                        node = edge["node"]
                        sha = node["oid"]
                        
                        if sha in known_shas: continue # å†…å­˜çº§å»é‡
                        
                        author_info = node.get("author", {})
                        user_node = author_info.get("user") or {}
                        
                        record = CommitRecord(
                            repo_name=repo_name,
                            commit_sha=sha,
                            timestamp_unix=int(datetime.fromisoformat(node["committedDate"].replace("Z", "+00:00")).timestamp()),
                            author_login=user_node.get("login") or "ghost",
                            author_name=author_info.get("name") or "Unknown",
                            author_email=author_info.get("email") or "",
                            location=user_node.get("location") or "",
                            message=node["message"][:500]
                        )
                        batch.append(record)
                        known_shas.add(sha)
                    
                    if batch:
                        await self.data_queue.put(batch)
                        pbar.update(len(batch))

                    page_info = history.get("pageInfo", {})
                    if page_info.get("hasNextPage") and self.is_running:
                        variables["cursor"] = page_info.get("endCursor")
                    else:
                        break
                except Exception as e:
                    self._logger.error(f"è§£æ {repo_name} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
                    break

    async def storage_worker(self):
        """æ¶ˆè´¹è€…ï¼šè´Ÿè´£å°†é˜Ÿåˆ—ä¸­çš„æ•°æ®æ‰¹é‡åˆ·å…¥ç£ç›˜"""
        buffer = []
        while self.is_running or not self.data_queue.empty():
            try:
                # å¸¦æœ‰è¶…æ—¶çš„ç­‰å¾…ï¼Œç¡®ä¿åœ¨åœæ­¢ä»»åŠ¡æ—¶èƒ½åŠæ—¶å“åº”
                batch = await asyncio.wait_for(self.data_queue.get(), timeout=2.0)
                buffer.extend(batch)
                
                if len(buffer) >= AppConfig.WRITE_BATCH_SIZE:
                    await self.db.save_batch(buffer)
                    self.stats["total_saved"] += len(buffer)
                    buffer = []
                
                self.data_queue.task_done()
            except asyncio.TimeoutError:
                if buffer:
                    await self.db.save_batch(buffer)
                    self.stats["total_saved"] += len(buffer)
                    buffer = []
                continue

# ======================== ä»»åŠ¡ç®¡ç†ä¸å…¥å£ ========================

class Application:
    def __init__(self, repos: List[str], tokens: List[str], days: int):
        self.repos = repos
        self.tokens = tokens
        self.days = days
        self.db = AsyncDatabase(AppConfig.DB_PATH)
        self.tp = TokenPool(tokens)
        self.engine = CollectionEngine(self.tp, self.db)

    def _setup_logging(self):
        logging.basicConfig(level=logging.INFO, format=AppConfig.LOG_FORMAT)

    async def run(self):
        self._setup_logging()
        await self.db.connect()
        
        # æ—¶é—´çª—å£
        until = datetime.now(timezone.utc)
        since = until - timedelta(days=self.days)
        
        # ä¿¡å·å¤„ç†
        loop = asyncio.get_running_loop()
        for sig in (signal.SIGINT, signal.SIGTERM):
            loop.add_signal_handler(sig, lambda: setattr(self.engine, 'is_running', False))

        print(f"ğŸš€ ä»»åŠ¡å¯åŠ¨ | ç›®æ ‡ä»“åº“: {len(self.repos)} | è¿½æº¯æ—¶é•¿: {self.days}å¤©")
        
        # å¯åŠ¨æŒä¹…åŒ–æ¶ˆè´¹è€…
        storage_task = asyncio.create_task(self.engine.storage_worker())
        
        # å¯åŠ¨å¹¶å‘ç”Ÿäº§è€…
        start_time = time.time()
        semaphore = asyncio.Semaphore(AppConfig.CONCURRENT_REPOS)
        
        async def sem_worker(repo, pbar):
            async with semaphore:
                await self.engine.repository_worker(repo, since, until, pbar)

        with tqdm(desc="æ•°æ®æŠ“å–è¿›åº¦", unit="æ¡") as pbar:
            tasks = [sem_worker(repo, pbar) for repo in self.repos]
            await asyncio.gather(*tasks)

        # ç­‰å¾…æ•°æ®å…¨éƒ¨è½ç›˜
        self.engine.is_running = False
        await storage_task
        await self.db.close()
        
        elapsed = time.time() - start_time
        print(f"\nâœ… é‡‡é›†å®Œæˆ! ")
        print(f"æ€»è®¡æŒä¹…åŒ–: {self.engine.stats['total_saved']} æ¡è®°å½•")
        print(f"æœ‰æ•ˆè€—æ—¶: {elapsed:.2f} ç§’")
        print(f"å¹³å‡åå: {self.engine.stats['total_saved']/elapsed:.1f} æ¡/ç§’")

# ======================== å¯åŠ¨é€»è¾‘ ========================

async def main():
    import argparse
    parser = argparse.ArgumentParser(description="GitHub Enterprise Collector v3")
    parser.add_argument("--repos", nargs="+", required=True)
    parser.add_argument("--tokens", nargs="+", required=True)
    parser.add_argument("--days", type=int, default=30)
    args = parser.parse_args()

    app = Application(args.repos, args.tokens, args.days)
    await app.run()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        pass
