# -*- coding: utf-8 -*-
"""
Gemini åœ°ç†ä¿¡æ¯æ ‡å‡†åŒ–ä¸“ä¸šç‰ˆ (GeoStandardizer Pro)

åŠŸèƒ½æè¿°:
    åˆ©ç”¨ Google Gemini API å°†éç»“æ„åŒ–çš„åœ°ååˆ—è¡¨æ¸…æ´—ä¸ºæ ‡å‡†çš„ç»“æ„åŒ–åœ°ç†æ•°æ®ã€‚
    å…·å¤‡é«˜æ€§èƒ½å¹¶å‘ã€æœ¬åœ° SQLite ç¼“å­˜ã€æ–­ç‚¹ç»­ä¼ åŠè¯¦ç»†çš„ç»Ÿè®¡æŠ¥å‘ŠåŠŸèƒ½ã€‚

æ¶æ„è®¾è®¡:
    - Config: é…ç½®ç®¡ç†
    - StorageEngine: æ•°æ®æŒä¹…åŒ–å±‚ (SQLite)
    - GeminiClient: API äº¤äº’å±‚ (Async HTTP)
    - IOHandler: æ–‡ä»¶è¾“å…¥è¾“å‡ºå¤„ç†
    - BatchProcessor: æ ¸å¿ƒå·¥ä½œæµæ§åˆ¶å™¨

ä¾èµ–:
    pip install aiohttp tqdm
"""

import asyncio
import csv
import json
import logging
import os
import signal
import sqlite3
import sys
import time
from argparse import ArgumentParser, RawTextHelpFormatter
from dataclasses import dataclass, field, asdict
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any, Optional, Set, Tuple, Generator, Iterable

# ======================== ä¾èµ–æ£€æŸ¥ ========================
try:
    import aiohttp
    from tqdm.asyncio import tqdm
except ImportError:
    sys.stderr.write("âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦ä¾èµ–ã€‚\nè¯·è¿è¡Œ: pip install aiohttp tqdm\n")
    sys.exit(1)

# ======================== å¸¸é‡ä¸ Schema ========================

API_BASE_URL = "https://generativelanguage.googleapis.com/v1beta/models"
ENV_API_KEY_NAME = "GEMINI_API_KEY"

# å®šä¹‰ä¸¥æ ¼çš„è¾“å‡º Schemaï¼Œç¡®ä¿æ¨¡å‹è¿”å›æ ¼å¼å¯æ§
LOCATION_RESPONSE_SCHEMA = {
    "type": "ARRAY",
    "items": {
        "type": "OBJECT",
        "properties": {
            "input_location": {"type": "STRING", "description": "åŸå§‹è¾“å…¥æ–‡æœ¬"},
            "city": {"type": "STRING", "description": "åŸå¸‚æˆ–åœ°æ–¹åç§°"},
            "subdivision": {"type": "STRING", "description": "çœã€å·æˆ–ä¸€çº§è¡Œæ”¿åŒº"},
            "country_alpha2": {"type": "STRING", "description": "ISO 3166-1 Alpha-2 ä»£ç "},
            "country_alpha3": {"type": "STRING", "description": "ISO 3166-1 Alpha-3 ä»£ç "},
            "confidence": {"type": "NUMBER", "description": "0.0-1.0 ä¹‹é—´çš„ç½®ä¿¡åº¦"},
            "reasoning": {"type": "STRING", "description": "ç®€çŸ­çš„æ¨æ–­ä¾æ®"}
        },
        "required": ["input_location", "country_alpha3", "confidence"]
    }
}

SYSTEM_PROMPT = (
    "æ‚¨æ˜¯ä¸€ä¸ªé«˜ç²¾åº¦çš„åœ°ç†ä¿¡æ¯æ ‡å‡†åŒ–å¼•æ“ã€‚\n"
    "ä»»åŠ¡ï¼šå°†è¾“å…¥çš„åœ°ç†æè¿°åˆ—è¡¨è½¬æ¢ä¸ºæ ‡å‡†çš„ç»“æ„åŒ–æ•°æ®ã€‚\n"
    "è§„åˆ™ï¼š\n"
    "1. ä¸¥æ ¼éµå®ˆ JSON Schemaï¼Œè¿”å› JSON æ•°ç»„ã€‚\n"
    "2. country_alpha3 å¿…é¡»ç¬¦åˆ ISO 3166-1 Alpha-3 æ ‡å‡†ã€‚\n"
    "3. å¦‚æœè¾“å…¥æ˜¯'California'ï¼Œsubdivision='California', country_alpha3='USA'ã€‚\n"
    "4. æ— æ³•è¯†åˆ«çš„è¾“å…¥ï¼Œcountry_alpha3='UNK'ï¼Œconfidence=0ã€‚\n"
    "5. ä¸è¦è¾“å‡º Markdown æ ‡è®°ï¼ˆå¦‚ ```jsonï¼‰ï¼Œä»…è¾“å‡ºçº¯æ–‡æœ¬ JSONã€‚"
)

# ======================== æ•°æ®æ¨¡å‹ä¸é…ç½® ========================

@dataclass
class AppConfig:
    """åº”ç”¨ç¨‹åºé…ç½®å¯¹è±¡"""
    input_path: Optional[str]
    output_path: str
    api_key: str
    model_name: str
    batch_size: int
    concurrency: int
    max_retries: int
    cache_db_path: str
    target_column: str
    is_demo: bool
    verbose: bool

@dataclass
class GeoRecord:
    """æ ‡å‡†åŒ–çš„åœ°ç†æ•°æ®è®°å½•"""
    input_location: str
    city: str = ""
    subdivision: str = ""
    country_alpha2: str = ""
    country_alpha3: str = "UNK"
    confidence: float = 0.0
    reasoning: str = ""
    updated_at: datetime = field(default_factory=datetime.now)

    def to_dict(self) -> Dict[str, Any]:
        d = asdict(self)
        d['updated_at'] = d['updated_at'].isoformat()
        return d

@dataclass
class Statistics:
    """è¿è¡Œæ—¶ç»Ÿè®¡ä¿¡æ¯"""
    total_inputs: int = 0
    unique_inputs: int = 0
    cached_hits: int = 0
    api_processed: int = 0
    api_errors: int = 0
    start_time: float = field(default_factory=time.time)

    @property
    def elapsed(self) -> float:
        return time.time() - self.start_time

    @property
    def speed(self) -> float:
        if self.elapsed < 0.1: return 0.0
        return self.total_inputs / self.elapsed

# ======================== æ—¥å¿—å·¥å…· ========================

def setup_logger(verbose: bool) -> logging.Logger:
    """é…ç½®å…¨å±€æ—¥å¿—"""
    logger = logging.getLogger("GeoStandardizer")
    level = logging.DEBUG if verbose else logging.INFO
    
    # æ¸…é™¤æ—§çš„ handlers
    if logger.handlers:
        logger.handlers.clear()
        
    formatter = logging.Formatter(
        "%(asctime)s | %(levelname)-8s | %(message)s", 
        datefmt="%H:%M:%S"
    )
    handler = logging.StreamHandler(sys.stdout)
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(level)
    return logger

# ======================== æ•°æ®æŒä¹…å±‚ ========================

class StorageEngine:
    """
    åŸºäº SQLite çš„ç¼“å­˜å¼•æ“ã€‚
    è´Ÿè´£æ•°æ®çš„å»é‡ã€ç¼“å­˜è¯»å–ä¸ç»“æœæŒä¹…åŒ–ã€‚
    """
    def __init__(self, db_path: str, logger: logging.Logger):
        self.db_path = db_path
        self.logger = logger
        self._conn: Optional[sqlite3.Connection] = None
        self._init_schema()

    def _get_conn(self) -> sqlite3.Connection:
        if not self._conn:
            self._conn = sqlite3.connect(self.db_path, check_same_thread=False)
            # å¼€å¯ WAL æ¨¡å¼ä»¥æé«˜å¹¶å‘è¯»å†™æ€§èƒ½
            self._conn.execute("PRAGMA journal_mode=WAL;")
        return self._conn

    def _init_schema(self):
        with self._get_conn() as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS geo_cache (
                    input_text TEXT PRIMARY KEY,
                    city TEXT,
                    subdivision TEXT,
                    country_alpha2 TEXT,
                    country_alpha3 TEXT,
                    confidence REAL,
                    reasoning TEXT,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            conn.commit()

    def get_cached_records(self, inputs: List[str]) -> Dict[str, GeoRecord]:
        """æ‰¹é‡è·å–ç¼“å­˜æ•°æ®"""
        if not inputs:
            return {}
        
        results = {}
        conn = self._get_conn()
        
        # SQLite é»˜è®¤å˜é‡é™åˆ¶é€šå¸¸æ˜¯ 999 æˆ– 32766ï¼Œåˆ†å—æŸ¥è¯¢æ›´å®‰å…¨
        chunk_size = 900
        for i in range(0, len(inputs), chunk_size):
            chunk = inputs[i:i + chunk_size]
            placeholders = ','.join(['?'] * len(chunk))
            try:
                cursor = conn.execute(
                    f"SELECT * FROM geo_cache WHERE input_text IN ({placeholders})", 
                    chunk
                )
                rows = cursor.fetchall()
                # è·å–åˆ—åæ˜ å°„
                cols = [desc[0] for desc in cursor.description]
                
                for row in rows:
                    data = dict(zip(cols, row))
                    # è½¬æ¢å› GeoRecord å¯¹è±¡
                    results[data['input_text']] = GeoRecord(
                        input_location=data['input_text'],
                        city=data.get('city', ''),
                        subdivision=data.get('subdivision', ''),
                        country_alpha2=data.get('country_alpha2', ''),
                        country_alpha3=data.get('country_alpha3', 'UNK'),
                        confidence=data.get('confidence', 0.0),
                        reasoning=data.get('reasoning', ''),
                        updated_at=data['updated_at'] # ä¿æŒåŸå§‹å­—ç¬¦ä¸²æˆ–è½¬æ¢çš†å¯ï¼Œæ­¤å¤„ä¸»è¦ç”¨äºå±•ç¤º
                    )
            except sqlite3.Error as e:
                self.logger.error(f"æ•°æ®åº“è¯»å–é”™è¯¯: {e}")

        return results

    def save_batch(self, records: List[GeoRecord]):
        """æ‰¹é‡å†™å…¥æˆ–æ›´æ–°ç¼“å­˜"""
        if not records:
            return

        sql = """
            INSERT OR REPLACE INTO geo_cache 
            (input_text, city, subdivision, country_alpha2, country_alpha3, confidence, reasoning, updated_at) 
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """
        data = [
            (
                r.input_location, r.city, r.subdivision, r.country_alpha2, 
                r.country_alpha3, r.confidence, r.reasoning, datetime.now()
            ) 
            for r in records
        ]
        
        try:
            conn = self._get_conn()
            conn.executemany(sql, data)
            conn.commit()
        except sqlite3.Error as e:
            self.logger.error(f"æ•°æ®åº“å†™å…¥é”™è¯¯: {e}")

    def close(self):
        if self._conn:
            self._conn.close()
            self._conn = None

# ======================== API å®¢æˆ·ç«¯ ========================

class GeminiClient:
    """
    Gemini API äº¤äº’å®¢æˆ·ç«¯ã€‚
    å¤„ç†è¯·æ±‚æ„å»ºã€é‡è¯•é€»è¾‘åŠé”™è¯¯è§£æã€‚
    """
    def __init__(self, config: AppConfig, logger: logging.Logger):
        self.config = config
        self.logger = logger
        self.endpoint = f"{API_BASE_URL}/{config.model_name}:generateContent?key={config.api_key}"

    def _build_payload(self, batch_inputs: List[str]) -> Dict:
        prompt_text = f"è¯·æ ‡å‡†åŒ–ä»¥ä¸‹åœ°ç‚¹æ¸…å•ï¼š\n{json.dumps(batch_inputs, ensure_ascii=False)}"
        return {
            "contents": [{"parts": [{"text": prompt_text}]}],
            "systemInstruction": {"parts": [{"text": SYSTEM_PROMPT}]},
            "generationConfig": {
                "responseMimeType": "application/json",
                "responseSchema": LOCATION_RESPONSE_SCHEMA
            }
        }

    async def standardize_batch(self, session: aiohttp.ClientSession, batch: List[str]) -> List[GeoRecord]:
        """
        å‘é€ API è¯·æ±‚å¹¶è¿”å›è§£æåçš„è®°å½•åˆ—è¡¨ã€‚
        åŒ…å«æŒ‡æ•°é€€é¿é‡è¯•æœºåˆ¶ã€‚
        """
        payload = self._build_payload(batch)
        
        for attempt in range(self.config.max_retries):
            try:
                async with session.post(self.endpoint, json=payload, timeout=60) as response:
                    if response.status == 429:
                        wait_time = (2 ** attempt) + 1
                        self.logger.debug(f"API é™æµ (429)ï¼Œä¼‘çœ  {wait_time}s åé‡è¯•...")
                        await asyncio.sleep(wait_time)
                        continue

                    if response.status != 200:
                        error_msg = await response.text()
                        self.logger.warning(f"API é”™è¯¯ [{response.status}]: {error_msg[:100]}...")
                        # 5xx é”™è¯¯æ‰é‡è¯•ï¼Œ4xx ç›´æ¥è·³è¿‡
                        if 500 <= response.status < 600:
                            await asyncio.sleep(2 ** attempt)
                            continue
                        else:
                            break 

                    data = await response.json()
                    return self._parse_response(data, batch)

            except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                self.logger.warning(f"ç½‘ç»œå¼‚å¸¸ (å°è¯• {attempt+1}/{self.config.max_retries}): {e}")
                await asyncio.sleep(2 ** attempt)

        # æœ€ç»ˆå¤±è´¥ï¼Œè¿”å› fallback è®°å½•
        self.logger.error(f"æ‰¹æ¬¡å¤„ç†å¤±è´¥: {batch[:3]}...")
        return [self._create_fallback_record(loc, "API processing failed") for loc in batch]

    def _parse_response(self, api_response: Dict, original_batch: List[str]) -> List[GeoRecord]:
        """è§£æ API è¿”å›çš„ JSONï¼Œå¹¶å¤„ç†å¯èƒ½çš„é—æ¼æˆ–æ ¼å¼é”™è¯¯"""
        records = []
        try:
            candidates = api_response.get('candidates', [])
            if not candidates:
                raise ValueError("No candidates returned")
            
            content_text = candidates[0]['content']['parts'][0]['text']
            raw_data = json.loads(content_text)
            
            # å»ºç«‹æ˜ å°„ä»¥é˜²ä¹±åº
            result_map = {item['input_location']: item for item in raw_data}
            
            for input_loc in original_batch:
                if input_loc in result_map:
                    item = result_map[input_loc]
                    records.append(GeoRecord(
                        input_location=item['input_location'],
                        city=item.get('city', ''),
                        subdivision=item.get('subdivision', ''),
                        country_alpha2=item.get('country_alpha2', ''),
                        country_alpha3=item.get('country_alpha3', 'UNK'),
                        confidence=item.get('confidence', 0),
                        reasoning=item.get('reasoning', '')
                    ))
                else:
                    records.append(self._create_fallback_record(input_loc, "Model skipped item"))
                    
        except Exception as e:
            self.logger.error(f"å“åº”è§£æå¤±è´¥: {e}")
            # è§£æå¤±è´¥åˆ™å…¨éƒ¨å›é€€
            return [self._create_fallback_record(loc, f"Parse error: {str(e)}") for loc in original_batch]
            
        return records

    @staticmethod
    def _create_fallback_record(location: str, reason: str) -> GeoRecord:
        return GeoRecord(
            input_location=location,
            country_alpha3="ERROR",
            reasoning=reason
        )

# ======================== IO å¤„ç† ========================

class IOHandler:
    """è´Ÿè´£æ–‡ä»¶çš„è¯»å–å’Œç»“æœå¯¼å‡º"""
    
    @staticmethod
    def read_input(path_str: Optional[str], column: str, is_demo: bool) -> List[str]:
        if is_demo:
            return ["New York", "London", "ä¸Šæµ·", "Tokyo", "Berlin", "Paris", "California", "UnknownCity123"] * 5
        
        if not path_str:
            raise ValueError("æœªæŒ‡å®šè¾“å…¥æ–‡ä»¶")

        path = Path(path_str)
        if not path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {path}")

        data = []
        # å¤„ç† CSV
        if path.suffix.lower() == '.csv':
            try:
                with open(path, 'r', encoding='utf-8-sig') as f:
                    reader = csv.DictReader(f)
                    if not reader.fieldnames:
                        raise ValueError("CSV ä¸ºç©ºæˆ–æ— è¡¨å¤´")
                    
                    # æ™ºèƒ½åˆ¤æ–­åˆ—åï¼šå¦‚æœæŒ‡å®šçš„åˆ—ä¸å­˜åœ¨ï¼Œå°è¯•ä½¿ç”¨ç¬¬ä¸€åˆ—
                    target_col = column if column in reader.fieldnames else reader.fieldnames[0]
                    
                    for row in reader:
                        if val := row.get(target_col):
                            data.append(str(val).strip())
            except UnicodeDecodeError:
                raise ValueError("æ–‡ä»¶ç¼–ç é”™è¯¯ï¼Œè¯·ä½¿ç”¨ UTF-8")
        
        # å¤„ç† JSON
        elif path.suffix.lower() == '.json':
            with open(path, 'r', encoding='utf-8') as f:
                content = json.load(f)
                if isinstance(content, list):
                    data = [str(x) for x in content]
                elif isinstance(content, dict):
                    data = [str(v) for v in content.values()]
        else:
            # é»˜è®¤æŒ‰è¡Œè¯»å–æ–‡æœ¬
            with open(path, 'r', encoding='utf-8') as f:
                data = [line.strip() for line in f if line.strip()]

        return [d for d in data if d]

    @staticmethod
    def export_results(results: List[GeoRecord], output_path: str):
        path = Path(output_path)
        dicts = [r.to_dict() for r in results]
        fieldnames = ["input_location", "city", "subdivision", "country_alpha2", "country_alpha3", "confidence", "reasoning", "updated_at"]
        
        path.parent.mkdir(parents=True, exist_ok=True)
        
        if path.suffix.lower() == '.json':
            with open(path, 'w', encoding='utf-8') as f:
                json.dump(dicts, f, ensure_ascii=False, indent=2)
        else:
            # é»˜è®¤å¯¼å‡º CSV
            with open(path, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                writer.writeheader()
                writer.writerows(dicts)

# ======================== æ ¸å¿ƒæ§åˆ¶å™¨ ========================

class BatchProcessor:
    """
    æ ¸å¿ƒå·¥ä½œæµæ§åˆ¶å™¨ã€‚
    åè°ƒ IOã€ç¼“å­˜ã€API å’Œå¹¶å‘å¤„ç†ã€‚
    """
    def __init__(self, config: AppConfig):
        self.config = config
        self.logger = setup_logger(config.verbose)
        self.stats = Statistics()
        self.storage = StorageEngine(config.cache_db_path, self.logger)
        self.client = GeminiClient(config, self.logger)
        self.is_running = True
        
        # ä¿¡å·æ³¨å†Œ
        signal.signal(signal.SIGINT, self._shutdown)
        signal.signal(signal.SIGTERM, self._shutdown)

    def _shutdown(self, sig, frame):
        if self.is_running:
            self.logger.warning("\nğŸ›‘ æ¥æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦å¹¶ä¼˜é›…é€€å‡º...")
            self.is_running = False

    def _batch_generator(self, data: List[str], batch_size: int) -> Generator[List[str], None, None]:
        for i in range(0, len(data), batch_size):
            yield data[i:i + batch_size]

    async def _process_worker(self, session: aiohttp.ClientSession, batch: List[str], semaphore: asyncio.Semaphore, pbar: tqdm):
        """å•ä¸ªå·¥ä½œåç¨‹ï¼šè¯·æ±‚ API -> ä¿å­˜ DB -> æ›´æ–°è¿›åº¦æ¡"""
        async with semaphore:
            if not self.is_running: return

            records = await self.client.standardize_batch(session, batch)
            
            # ç»Ÿè®¡æˆåŠŸä¸å¤±è´¥
            valid_count = sum(1 for r in records if r.country_alpha3 != 'ERROR')
            self.stats.api_processed += valid_count
            self.stats.api_errors += (len(records) - valid_count)

            # æŒä¹…åŒ–
            self.storage.save_batch(records)
            pbar.update(len(batch))

    async def run(self):
        try:
            # 1. åŠ è½½æ•°æ®
            self.logger.info("æ­£åœ¨è¯»å–è¾“å…¥æ•°æ®...")
            raw_inputs = IOHandler.read_input(self.config.input_path, self.config.target_column, self.config.is_demo)
            
            self.stats.total_inputs = len(raw_inputs)
            unique_inputs = list(dict.fromkeys(raw_inputs)) # ä¿æŒé¡ºåºå»é‡
            self.stats.unique_inputs = len(unique_inputs)

            if not raw_inputs:
                self.logger.warning("è¾“å…¥æ•°æ®ä¸ºç©ºï¼Œä»»åŠ¡ç»“æŸã€‚")
                return

            # 2. æ£€æŸ¥ç¼“å­˜
            self.logger.info("æ­£åœ¨æ¯”å¯¹æœ¬åœ°ç¼“å­˜...")
            cached_map = self.storage.get_cached_records(unique_inputs)
            self.stats.cached_hits = len(cached_map)
            
            # ç­›é€‰å¾…å¤„ç†åˆ—è¡¨
            to_process = [loc for loc in unique_inputs if loc not in cached_map]
            
            self.logger.info(
                f"ä»»åŠ¡æ¦‚è§ˆ | æ€»é‡: {self.stats.total_inputs} | å”¯ä¸€: {self.stats.unique_inputs} | "
                f"å·²ç¼“å­˜: {self.stats.cached_hits} | å¾…è¯·æ±‚: {len(to_process)}"
            )

            # 3. å¹¶å‘å¤„ç†
            if to_process:
                concurrency_sem = asyncio.Semaphore(self.config.concurrency)
                batches = list(self._batch_generator(to_process, self.config.batch_size))
                
                async with aiohttp.ClientSession() as session:
                    tasks = []
                    with tqdm(total=len(to_process), desc="API å¤„ç†è¿›åº¦", unit="loc") as pbar:
                        for batch in batches:
                            if not self.is_running: break
                            task = asyncio.create_task(
                                self._process_worker(session, batch, concurrency_sem, pbar)
                            )
                            tasks.append(task)
                        
                        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                        await asyncio.gather(*tasks)
            
            # 4. ç»“æœæ•´åˆä¸å¯¼å‡º
            if self.is_running:
                self.logger.info("æ­£åœ¨æ•´åˆæœ€ç»ˆç»“æœ...")
                # é‡æ–°è·å–æ‰€æœ‰æ•°æ®çš„å®Œæ•´è®°å½•
                final_cache = self.storage.get_cached_records(list(set(raw_inputs)))
                final_results = [
                    final_cache.get(loc, GeoRecord(input_location=loc, country_alpha3="MISSING")) 
                    for loc in raw_inputs
                ]
                
                IOHandler.export_results(final_results, self.config.output_path)
                self._print_summary()

        except Exception as e:
            self.logger.error(f"ä¸¥é‡è¿è¡Œæ—¶é”™è¯¯: {e}", exc_info=self.config.verbose)
        finally:
            self.storage.close()

    def _print_summary(self):
        self.logger.info("=" * 40)
        self.logger.info("âœ… å¤„ç†å®Œæˆ")
        self.logger.info(f"è€—æ—¶: {self.stats.elapsed:.2f} ç§’")
        self.logger.info(f"å¹³å‡å¤„ç†é€Ÿåº¦: {self.stats.speed:.1f} æ¡/ç§’")
        self.logger.info(f"API è°ƒç”¨æˆåŠŸ: {self.stats.api_processed} | å¤±è´¥: {self.stats.api_errors}")
        self.logger.info(f"ç»“æœå·²ä¿å­˜è‡³: {Path(self.config.output_path).absolute()}")
        self.logger.info("=" * 40)

# ======================== å…¥å£å‡½æ•° ========================

def main():
    parser = ArgumentParser(description="Gemini åœ°ç†æ ‡å‡†åŒ–å¼•æ“ (ä¸“ä¸šç‰ˆ)", formatter_class=RawTextHelpFormatter)
    
    # åŸºç¡€å‚æ•°
    base_group = parser.add_argument_group("åŸºç¡€è®¾ç½®")
    base_group.add_argument("--input", "-i", help="è¾“å…¥æ–‡ä»¶è·¯å¾„ (æ”¯æŒ CSV, JSON, TXT)")
    base_group.add_argument("--output", "-o", default="geo_output_pro.csv", help="ç»“æœè¾“å‡ºè·¯å¾„")
    base_group.add_argument("--key", "-k", default=os.environ.get(ENV_API_KEY_NAME), help=f"API Key (é»˜è®¤è¯»å–ç¯å¢ƒå˜é‡ {ENV_API_KEY_NAME})")
    
    # æ€§èƒ½å‚æ•°
    perf_group = parser.add_argument_group("æ€§èƒ½è°ƒä¼˜")
    perf_group.add_argument("--model", default="gemini-2.5-flash-preview-09-2025", help="ä½¿ç”¨çš„ Gemini æ¨¡å‹ç‰ˆæœ¬")
    perf_group.add_argument("--batch", "-b", type=int, default=30, help="æ‰¹å¤„ç†å¤§å° (æ¨è 20-50)")
    perf_group.add_argument("--concurrency", "-c", type=int, default=5, help="å¹¶å‘åç¨‹æ•°")
    perf_group.add_argument("--retry", type=int, default=3, help="API å¤±è´¥é‡è¯•æ¬¡æ•°")
    
    # å…¶ä»–é€‰é¡¹
    misc_group = parser.add_argument_group("å…¶ä»–é€‰é¡¹")
    misc_group.add_argument("--cache", default="geo_cache_v2.db", help="SQLite ç¼“å­˜æ•°æ®åº“è·¯å¾„")
    misc_group.add_argument("--column", default="location", help="CSV ä¸­çš„ç›®æ ‡åˆ—å")
    misc_group.add_argument("--demo", action="store_true", help="ä½¿ç”¨å†…ç½®æµ‹è¯•æ•°æ®è¿è¡Œ")
    misc_group.add_argument("--verbose", "-v", action="store_true", help="å¯ç”¨è¯¦ç»†è°ƒè¯•æ—¥å¿—")

    args = parser.parse_args()

    # å‚æ•°æ ¡éªŒ
    if not args.key:
        parser.error(f"æœªæä¾› API Keyã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡ {ENV_API_KEY_NAME} æˆ–ä½¿ç”¨ --key å‚æ•°ã€‚")
    
    if not args.input and not args.demo:
        parser.error("éœ€è¦æä¾›è¾“å…¥æ–‡ä»¶ (--input) æˆ–ä½¿ç”¨æ¼”ç¤ºæ¨¡å¼ (--demo)ã€‚")

    # æ„å»ºé…ç½®å¯¹è±¡
    config = AppConfig(
        input_path=args.input,
        output_path=args.output,
        api_key=args.key,
        model_name=args.model,
        batch_size=args.batch,
        concurrency=args.concurrency,
        max_retries=args.retry,
        cache_db_path=args.cache,
        target_column=args.column,
        is_demo=args.demo,
        verbose=args.verbose
    )

    # å¯åŠ¨åº”ç”¨
    processor = BatchProcessor(config)
    try:
        asyncio.run(processor.run())
    except KeyboardInterrupt:
        pass  # å·²ç”±ä¿¡å·å¤„ç†ç¨‹åºå¤„ç†ï¼Œæ­¤å¤„åªéœ€æ•è·ä»¥é¿å…æ‰“å°å †æ ˆ

if __name__ == "__main__":
    main()
